{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Federated Learning with Edge IIoT Dataset using Flower and TensorFlow/Keras\n",
    "\n",
    "In this notebook we use the Flower Federated Learning library (flower.dev) with scikit-learn to distribute the Edge-IIoT data across multiple clients in various different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDIVIDUAL_IP\n"
     ]
    }
   ],
   "source": [
    "### THIS SECTION NEEDS TO BE SET TO DETERMINE WHICH CONFIGURATION METHOD TO UTILISE\n",
    "\n",
    "AVAILABLE_METHODS = ['INDIVIDUAL_ATTACK', 'ATTACK_GROUP', 'STRATIFIED']\n",
    "METHOD = AVAILABLE_METHODS[0]\n",
    "NUM_OF_STRATIFIED_CLIENTS = 10 # only applies to stratified method\n",
    "print (METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install flwr[simulation] torch torchvision matplotlib sklearn openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from flwr.common import Metrics\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flwr 1.3.0\n",
      "numpy 1.24.2\n",
      "torch 1.13.1\n",
      "torchvision 0.14.1\n",
      "Training on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"torchvision\", torchvision.__version__)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../datasets/Edge-IIoT/\"\n",
    "\n",
    "df = pd.read_pickle(dataset_path + \"Edge-IIoTset dataset/Selected dataset for ML and DL/ML-EdgeIIoT-dataset.pkl\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make two dataframes for 2 models. Binary classification and Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary attack dataframe\n",
    "binary_df=df[['frame.time', 'ip.src_host', 'ip.dst_host', 'arp.dst.proto_ipv4',\n",
    "       'arp.opcode', 'arp.hw.size', 'arp.src.proto_ipv4', 'icmp.checksum',\n",
    "       'icmp.seq_le', 'icmp.transmit_timestamp', 'icmp.unused',\n",
    "       'http.file_data', 'http.content_length', 'http.request.uri.query',\n",
    "       'http.request.method', 'http.referer', 'http.request.full_uri',\n",
    "       'http.request.version', 'http.response', 'http.tls_port', 'tcp.ack',\n",
    "       'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin',\n",
    "       'tcp.connection.rst', 'tcp.connection.syn', 'tcp.connection.synack',\n",
    "       'tcp.dstport', 'tcp.flags', 'tcp.flags.ack', 'tcp.len', 'tcp.options',\n",
    "       'tcp.payload', 'tcp.seq', 'tcp.srcport', 'udp.port', 'udp.stream',\n",
    "       'udp.time_delta', 'dns.qry.name', 'dns.qry.name.len', 'dns.qry.qu',\n",
    "       'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request',\n",
    "       'dns.retransmit_request_in', 'mqtt.conack.flags',\n",
    "       'mqtt.conflag.cleansess', 'mqtt.conflags', 'mqtt.hdrflags', 'mqtt.len',\n",
    "       'mqtt.msg_decoded_as', 'mqtt.msg', 'mqtt.msgtype', 'mqtt.proto_len',\n",
    "       'mqtt.protoname', 'mqtt.topic', 'mqtt.topic_len', 'mqtt.ver',\n",
    "       'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id', 'Attack_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass attack dataframe\n",
    "multiclass_df = df[['frame.time', 'ip.src_host', 'ip.dst_host', 'arp.dst.proto_ipv4',\n",
    "       'arp.opcode', 'arp.hw.size', 'arp.src.proto_ipv4', 'icmp.checksum',\n",
    "       'icmp.seq_le', 'icmp.transmit_timestamp', 'icmp.unused',\n",
    "       'http.file_data', 'http.content_length', 'http.request.uri.query',\n",
    "       'http.request.method', 'http.referer', 'http.request.full_uri',\n",
    "       'http.request.version', 'http.response', 'http.tls_port', 'tcp.ack',\n",
    "       'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin',\n",
    "       'tcp.connection.rst', 'tcp.connection.syn', 'tcp.connection.synack',\n",
    "       'tcp.dstport', 'tcp.flags', 'tcp.flags.ack', 'tcp.len', 'tcp.options',\n",
    "       'tcp.payload', 'tcp.seq', 'tcp.srcport', 'udp.port', 'udp.stream',\n",
    "       'udp.time_delta', 'dns.qry.name', 'dns.qry.name.len', 'dns.qry.qu',\n",
    "       'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request',\n",
    "       'dns.retransmit_request_in', 'mqtt.conack.flags',\n",
    "       'mqtt.conflag.cleansess', 'mqtt.conflags', 'mqtt.hdrflags', 'mqtt.len',\n",
    "       'mqtt.msg_decoded_as', 'mqtt.msg', 'mqtt.msgtype', 'mqtt.proto_len',\n",
    "       'mqtt.protoname', 'mqtt.topic', 'mqtt.topic_len', 'mqtt.ver',\n",
    "       'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id',\n",
    "       'Attack_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known_sensor_ip_addresses: ['192.168.0.101', '192.168.2.194', '192.168.3.18', '192.168.4.73', '192.168.5.47', '192.168.6.56', '192.768.7.62', '192.168.8.163']\n",
      "known_attacker_ip_addresses: \n",
      "Number of IPs 22\n",
      "['172.217.19.35', '133.149.252.77', '192.168.0.101', '192.168.0.170', '172.217.19.42', '142.250.200.205', '16.226.184.201', '166.153.227.121', '213.117.18.213', '91.184.12.91', '220.146.94.148', '142.250.201.10', '104.16.87.20', '153.125.214.15', '190.123.219.128', '183.223.100.122', '227.117.33.125', '207.192.25.133', '216.58.198.74', '192.168.0.152', '94.196.109.185', '49.81.59.152']\n"
     ]
    }
   ],
   "source": [
    "known_sensor_ip_addresses = [ '192.168.0.101', '192.168.2.194', '192.168.3.18', '192.168.4.73', '192.168.5.47', '192.168.6.56', '192.768.7.62', '192.168.8.163']\n",
    "print (\"known_sensor_ip_addresses:\", known_sensor_ip_addresses)\n",
    "\n",
    "tcp_dos_attack_ip_addresses = [ '207.192.25.133', '94.196.109.185', '133.149.252.77', '220.146.94.148' ]\n",
    "tdp_dos_atack_ip_addresses = [ '190.123.219.128', '16.226.184.201', '153.125.214.15', '91.184.12.91' ]\n",
    "http_attack_ip_addresses = [ '192.168.0.170', '216.58.198.74' ]\n",
    "icmp_flood_attack_ip_addresses = [ '213.117.18.213', '183.223.100.122', '166.153.227.121', '49.81.59.152', '227.117.33.125' ]\n",
    "port_scan_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "os_fingerprinting_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "vuln_scan_attack_ip_addresses = [ '192.168.0.170', '142.250.200.205', '172.217.19.35', '142.250.201.10' ]\n",
    "dns_spoof_attack_ip_addresses = [ '192.168.0.101', '192.168.0.152', '172.217.19.35', '192.168.0.170' ]\n",
    "arp_spoof_attack_ip_addresses = [ '192.168.0.101', '192.168.0.152', '172.217.19.35', '192.168.0.170' ]\n",
    "xss_attack_ip_addresses = [ '192.168.0.170', '172.217.19.42', '104.16.87.20' ]\n",
    "sql_injection_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "upload_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "backdoor_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "password_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "ransomware_attack_ip_addresses = [ '192.168.0.170' ] \n",
    "\n",
    "# Combine all attack IP addresses into one list, ensuring no duplicates\n",
    "known_attacker_ip_addresses = list(set(tcp_dos_attack_ip_addresses + tdp_dos_atack_ip_addresses + http_attack_ip_addresses + icmp_flood_attack_ip_addresses + port_scan_attack_ip_addresses + os_fingerprinting_attack_ip_addresses + vuln_scan_attack_ip_addresses + dns_spoof_attack_ip_addresses + arp_spoof_attack_ip_addresses + xss_attack_ip_addresses + sql_injection_attack_ip_addresses + upload_attack_ip_addresses + backdoor_attack_ip_addresses + password_attack_ip_addresses + ransomware_attack_ip_addresses))\n",
    "print (f\"known_attacker_ip_addresses: \\nNumber of IPs {len(known_attacker_ip_addresses)}\\n{known_attacker_ip_addresses}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exception of DDOS attacks, the attacks mainly come from a small subset of IP addresses. `192.168.0.170` being responsible for a lot of the attacks. From the data exploration workbook `02b-ML-Data-Exploration.ipynb` we can also see that the attacked IP is always the `192.168.0.128` edge server. This means it is not feasible the divide the traffic either by attacker or attacked IP address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical data encoding (Dummy Encoding):\n",
    "\n",
    "EG. Takes a product category and converts it to a binary vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_dummy(df, name):\n",
    "\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "\n",
    "    for x in dummies.columns:\n",
    "\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "\n",
    "        df[dummy_name] = dummies[x]\n",
    "\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "encode_text_dummy(binary_df,'http.request.method')\n",
    "\n",
    "encode_text_dummy(binary_df,'http.referer')\n",
    "\n",
    "encode_text_dummy(binary_df,\"http.request.version\")\n",
    "\n",
    "encode_text_dummy(binary_df,\"dns.qry.name.len\")\n",
    "\n",
    "encode_text_dummy(binary_df,\"mqtt.conack.flags\")\n",
    "\n",
    "encode_text_dummy(binary_df,\"mqtt.protoname\")\n",
    "\n",
    "encode_text_dummy(binary_df,\"mqtt.topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max index value of binary_df: 2219200\n",
      "(2219201, 111)\n"
     ]
    }
   ],
   "source": [
    "# print max index value of binary_df\n",
    "print(\"max index value of binary_df:\", max(binary_df.index))\n",
    "print(binary_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to drop some unrequired columns from the DF, but we need to keep the original around as we may need to split the data differently for different models based on things like the IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ip.src_host                            0\n",
       "ip.dst_host                            0\n",
       "arp.opcode                             0\n",
       "arp.hw.size                            0\n",
       "icmp.checksum                          0\n",
       "                                      ..\n",
       "mqtt.protoname-0.0                     0\n",
       "mqtt.protoname-MQTT                    0\n",
       "mqtt.topic-0                           0\n",
       "mqtt.topic-0.0                         0\n",
       "mqtt.topic-Temperature_and_Humidity    0\n",
       "Length: 98, dtype: int64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop_columns = [\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\",\"arp.dst.proto_ipv4\", \n",
    "drop_columns = [\"frame.time\", \"arp.src.proto_ipv4\",\"arp.dst.proto_ipv4\", \n",
    "\n",
    "         \"http.file_data\",\"http.request.full_uri\",\"icmp.transmit_timestamp\",\n",
    "\n",
    "         \"http.request.uri.query\", \"tcp.options\",\"tcp.payload\",\"tcp.srcport\",\n",
    "\n",
    "         \"tcp.dstport\", \"udp.port\", \"mqtt.msg\"]\n",
    "\n",
    "binary_df = binary_df.drop(drop_columns, axis=1)\n",
    "\n",
    "binary_df = binary_df.dropna(axis=0, how='any')\n",
    "\n",
    "binary_df = binary_df.drop_duplicates(subset=None, keep=\"first\")\n",
    "\n",
    "# We cant shuffle at this point as we need to keep the order so we can split the dataset later based on things like IP address\n",
    "#binary_df_copy = shuffle(binary_df_copy)\n",
    "\n",
    "# Compute the number of missing values (NaN or null) in each column of a pandas DataFrame object named df.\n",
    "binary_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max index value of binary_df: 2219200\n",
      "(1957467, 98)\n",
      "max index value of binary_df: 1957466\n",
      "(1957467, 98)\n"
     ]
    }
   ],
   "source": [
    "# print max index value of binary_df\n",
    "print(\"max index value of binary_df:\", max(binary_df.index))\n",
    "print(binary_df.shape)\n",
    "\n",
    "binary_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print max index value of binary_df\n",
    "print(\"max index value of binary_df:\", max(binary_df.index))\n",
    "print(binary_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1565973, 98)\n",
      "1565972\n",
      "1957466\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label = binary_df['Attack_label']\n",
    "le = preprocessing.LabelEncoder()\n",
    "label_n = le.fit_transform(label.values)\n",
    "\n",
    "# Stratify based on the attack label to balance the dataset - This is our original copy of the data include IP addresses\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(binary_df, label_n, stratify=label_n, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train_df.shape)\n",
    "\n",
    "X_train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#print the max index of X_train_df\n",
    "print(X_train_df.index.max())\n",
    "\n",
    "#  print the max index of binary_df\n",
    "print(binary_df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1565973, 98)\n",
      "1957466\n",
      "1957466\n"
     ]
    }
   ],
   "source": [
    "label = binary_df['Attack_label']\n",
    "le = preprocessing.LabelEncoder()\n",
    "label_n = le.fit_transform(label.values)\n",
    "\n",
    "# Stratify based on the attack label to balance the dataset - This is our original copy of the data include IP addresses\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(binary_df, label_n, stratify=label_n, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train_df.shape)\n",
    "\n",
    "#print the max index of X_train_df\n",
    "print(X_train_df.index.max())\n",
    "\n",
    "#  print the max index of binary_df\n",
    "print(binary_df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1565973, 95) (1565973,)\n",
      "Test: (391494, 95) (391494,)\n"
     ]
    }
   ],
   "source": [
    "binary_df_copy = binary_df.copy()\n",
    "\n",
    "binary_df_copy = binary_df_copy.drop([\"ip.src_host\", \"ip.dst_host\", \"Attack_label\"], axis=1)\n",
    "\n",
    "# This is our copy of the data without IP addresses\n",
    "scaled_features = StandardScaler().fit_transform(binary_df_copy.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, Y, stratify=Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print (\"Train:\", X_train.shape, y_train.shape)\n",
    "print (\"Test:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Attack classification\n",
    "N_CLASSES = 2\n",
    "# Number of features = size of binary_df - 1 (Attack_label)\n",
    "N_FEATURES = binary_df.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_ip: 192.168.0.101\n",
      "Shape X_Train: (1565973, 95)\n",
      "Shape y_Train: (1565973,)\n",
      "Shape new_df_src: (438417, 98)\n",
      "Shape new_df_dst: (436184, 98)\n",
      "x_np: (874601, 95)\n",
      "y_np: (874601,)\n",
      "new_ip: 192.168.2.194\n",
      "Shape X_Train: (1565973, 95)\n",
      "Shape y_Train: (1565973,)\n",
      "Shape new_df_src: (0, 98)\n",
      "Shape new_df_dst: (0, 98)\n",
      "x_np: (0, 95)\n",
      "y_np: (0,)\n",
      "new_ip: 192.168.3.18\n",
      "Shape X_Train: (1565973, 95)\n",
      "Shape y_Train: (1565973,)\n",
      "Shape new_df_src: (0, 98)\n",
      "Shape new_df_dst: (0, 98)\n",
      "x_np: (0, 95)\n",
      "y_np: (0,)\n",
      "new_ip: 192.168.4.73\n",
      "Shape X_Train: (1565973, 95)\n",
      "Shape y_Train: (1565973,)\n",
      "Shape new_df_src: (0, 98)\n",
      "Shape new_df_dst: (0, 98)\n",
      "x_np: (0, 95)\n",
      "y_np: (0,)\n",
      "new_ip: 192.168.5.47\n",
      "Shape X_Train: (1565973, 95)\n",
      "Shape y_Train: (1565973,)\n",
      "Shape new_df_src: (0, 98)\n",
      "Shape new_df_dst: (0, 98)\n",
      "x_np: (0, 95)\n",
      "y_np: (0,)\n",
      "new_ip: 192.168.6.56\n",
      "Shape X_Train: (1565973, 95)\n",
      "Shape y_Train: (1565973,)\n",
      "Shape new_df_src: (0, 98)\n",
      "Shape new_df_dst: (0, 98)\n",
      "x_np: (0, 95)\n",
      "y_np: (0,)\n",
      "new_ip: 192.768.7.62\n",
      "Shape X_Train: (1565973, 95)\n",
      "Shape y_Train: (1565973,)\n",
      "Shape new_df_src: (0, 98)\n",
      "Shape new_df_dst: (0, 98)\n",
      "x_np: (0, 95)\n",
      "y_np: (0,)\n",
      "new_ip: 192.168.8.163\n",
      "Shape X_Train: (1565973, 95)\n",
      "Shape y_Train: (1565973,)\n",
      "Shape new_df_src: (0, 98)\n",
      "Shape new_df_dst: (0, 98)\n",
      "x_np: (0, 95)\n",
      "y_np: (0,)\n"
     ]
    }
   ],
   "source": [
    "fl_X_train = []\n",
    "fl_Y_train = []\n",
    "\n",
    "if METHOD == AVAILABLE_METHODS[0]:\n",
    "    # Individual IP address\n",
    "    for ip in known_sensor_ip_addresses:\n",
    "        new_ip = [ip]\n",
    "        print(\"new_ip:\", new_ip)\n",
    "\n",
    "        X_train_df['ip.src_host']\n",
    "        \n",
    "        print(\"Shape X_Train:\", X_train.shape)\n",
    "        print(\"Shape y_Train:\", y_train.shape)\n",
    "\n",
    "        # Filter dataframe by IP address\n",
    "        new_df_src = X_train_df[ X_train_df['ip.src_host'].isin(new_ip) ]\n",
    "        new_df_dst = X_train_df[ X_train_df['ip.dst_host'].isin(new_ip) ]\n",
    "\n",
    "        print(\"Shape new_df_src:\", new_df_src.shape)\n",
    "        print(\"Shape new_df_dst:\", new_df_dst.shape)\n",
    "\n",
    "        X_np = np.vstack([ X_train[ new_df_src.index, : ], X_train[ new_df_dst.index, :] ])\n",
    "        y_np = np.hstack([ y_train[ new_df_src.index ], y_train[ new_df_dst.index ] ])\n",
    "\n",
    "        print (\"x_np:\", X_np.shape)\n",
    "        print (\"y_np:\", y_np.shape)\n",
    "\n",
    "        fl_X_train.append(X_np)\n",
    "        fl_Y_train.append(y_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union, List\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import openml\n",
    "\n",
    "XY = Tuple[np.ndarray, np.ndarray]\n",
    "Dataset = Tuple[XY, XY]\n",
    "LogRegParams = Union[XY, Tuple[np.ndarray]]\n",
    "XYList = List[XY]\n",
    "\n",
    "def get_model_parameters(model: LogisticRegression) -> LogRegParams:\n",
    "    \"\"\"Returns the paramters of a sklearn LogisticRegression model.\"\"\"\n",
    "    if model.fit_intercept:\n",
    "        params = [\n",
    "            model.coef_,\n",
    "            model.intercept_,\n",
    "        ]\n",
    "    else:\n",
    "        params = [\n",
    "            model.coef_,\n",
    "        ]\n",
    "    return params\n",
    "\n",
    "def set_model_params(\n",
    "    model: LogisticRegression, params: LogRegParams\n",
    ") -> LogisticRegression:\n",
    "    \"\"\"Sets the parameters of a sklean LogisticRegression model.\"\"\"\n",
    "    model.coef_ = params[0]\n",
    "    if model.fit_intercept:\n",
    "        model.intercept_ = params[1]\n",
    "    return model\n",
    "\n",
    "def set_initial_params(model: LogisticRegression):\n",
    "    \"\"\"Sets initial parameters as zeros Required since model params are\n",
    "    uninitialized until model.fit is called.\n",
    "    But server asks for initial parameters from clients at launch. Refer\n",
    "    to sklearn.linear_model.LogisticRegression documentation for more\n",
    "    information.\n",
    "    \"\"\"\n",
    "    n_classes = N_CLASSES\n",
    "    n_features = N_FEATURES\n",
    "    model.classes_ = np.array([i for i in range(10)])\n",
    "\n",
    "    model.coef_ = np.zeros((n_classes, n_features))\n",
    "    if model.fit_intercept:\n",
    "        model.intercept_ = np.zeros((n_classes,))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
