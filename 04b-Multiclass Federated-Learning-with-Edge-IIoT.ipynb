{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04b - Multiclass Federated Learning with Edge IIoT Dataset using Flower and TensorFlow/Keras\n",
    "\n",
    "In this notebook we use the Flower Federated Learning library (flower.dev) with Tensorflow/Keras to distribute the Edge-IIoT data across multiple clients in various different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS SECTION NEEDS TO BE SET TO DETERMINE WHICH CONFIGURATION METHOD TO UTILISE\n",
    "\n",
    "SPLIT_AVAILABLE_METHODS = ['INDIVIDUAL_ATTACK', 'ATTACK_GROUP', 'STRATIFIED']\n",
    "METHOD = 'STRATIFIED'\n",
    "NUM_OF_STRATIFIED_CLIENTS = 10 # only applies to stratified method\n",
    "print (METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install flwr[simulation] torch torchvision matplotlib sklearn openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from flwr.common import Metrics\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"torchvision\", torchvision.__version__)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../datasets/Edge-IIoT/\"\n",
    "\n",
    "df = pd.read_pickle(dataset_path + \"Edge-IIoTset dataset/Selected dataset for ML and DL/ML-EdgeIIoT-dataset.pkl\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataframe for Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass attack dataframe\n",
    "multiclass_df = df[['frame.time', 'ip.src_host', 'ip.dst_host', 'arp.dst.proto_ipv4',\n",
    "       'arp.opcode', 'arp.hw.size', 'arp.src.proto_ipv4', 'icmp.checksum',\n",
    "       'icmp.seq_le', 'icmp.transmit_timestamp', 'icmp.unused',\n",
    "       'http.file_data', 'http.content_length', 'http.request.uri.query',\n",
    "       'http.request.method', 'http.referer', 'http.request.full_uri',\n",
    "       'http.request.version', 'http.response', 'http.tls_port', 'tcp.ack',\n",
    "       'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin',\n",
    "       'tcp.connection.rst', 'tcp.connection.syn', 'tcp.connection.synack',\n",
    "       'tcp.dstport', 'tcp.flags', 'tcp.flags.ack', 'tcp.len', 'tcp.options',\n",
    "       'tcp.payload', 'tcp.seq', 'tcp.srcport', 'udp.port', 'udp.stream',\n",
    "       'udp.time_delta', 'dns.qry.name', 'dns.qry.name.len', 'dns.qry.qu',\n",
    "       'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request',\n",
    "       'dns.retransmit_request_in', 'mqtt.conack.flags',\n",
    "       'mqtt.conflag.cleansess', 'mqtt.conflags', 'mqtt.hdrflags', 'mqtt.len',\n",
    "       'mqtt.msg_decoded_as', 'mqtt.msg', 'mqtt.msgtype', 'mqtt.proto_len',\n",
    "       'mqtt.protoname', 'mqtt.topic', 'mqtt.topic_len', 'mqtt.ver',\n",
    "       'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id',\n",
    "       'Attack_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_sensor_ip_addresses = [ '192.168.0.101', '192.168.2.194', '192.168.3.18', '192.168.4.73', '192.168.5.47', '192.168.6.56', '192.768.7.62', '192.168.8.163']\n",
    "print (\"known_sensor_ip_addresses:\", known_sensor_ip_addresses)\n",
    "\n",
    "tcp_dos_attack_ip_addresses = [ '207.192.25.133', '94.196.109.185', '133.149.252.77', '220.146.94.148' ]\n",
    "tdp_dos_atack_ip_addresses = [ '190.123.219.128', '16.226.184.201', '153.125.214.15', '91.184.12.91' ]\n",
    "http_attack_ip_addresses = [ '192.168.0.170', '216.58.198.74' ]\n",
    "icmp_flood_attack_ip_addresses = [ '213.117.18.213', '183.223.100.122', '166.153.227.121', '49.81.59.152', '227.117.33.125' ]\n",
    "port_scan_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "os_fingerprinting_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "vuln_scan_attack_ip_addresses = [ '192.168.0.170', '142.250.200.205', '172.217.19.35', '142.250.201.10' ]\n",
    "dns_spoof_attack_ip_addresses = [ '192.168.0.101', '192.168.0.152', '172.217.19.35', '192.168.0.170' ]\n",
    "arp_spoof_attack_ip_addresses = [ '192.168.0.101', '192.168.0.152', '172.217.19.35', '192.168.0.170' ]\n",
    "xss_attack_ip_addresses = [ '192.168.0.170', '172.217.19.42', '104.16.87.20' ]\n",
    "sql_injection_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "upload_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "backdoor_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "password_attack_ip_addresses = [ '192.168.0.170' ]\n",
    "ransomware_attack_ip_addresses = [ '192.168.0.170' ] \n",
    "\n",
    "# Combine all attack IP addresses into one list, ensuring no duplicates\n",
    "known_attacker_ip_addresses = list(set(tcp_dos_attack_ip_addresses + tdp_dos_atack_ip_addresses + http_attack_ip_addresses + icmp_flood_attack_ip_addresses + port_scan_attack_ip_addresses + os_fingerprinting_attack_ip_addresses + vuln_scan_attack_ip_addresses + dns_spoof_attack_ip_addresses + arp_spoof_attack_ip_addresses + xss_attack_ip_addresses + sql_injection_attack_ip_addresses + upload_attack_ip_addresses + backdoor_attack_ip_addresses + password_attack_ip_addresses + ransomware_attack_ip_addresses))\n",
    "print (f\"known_attacker_ip_addresses: \\nNumber of IPs {len(known_attacker_ip_addresses)}\\n{known_attacker_ip_addresses}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exception of DDOS attacks, the attacks mainly come from a small subset of IP addresses. `192.168.0.170` being responsible for a lot of the attacks. From the data exploration workbook `02b-ML-Data-Exploration.ipynb` we can also see that the attacked IP is always the `192.168.0.128` edge server. This means it is not feasible the divide the traffic either by attacker or attacked IP address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical data encoding (Dummy Encoding):\n",
    "\n",
    "EG. Takes a product category and converts it to a binary vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_dummy(df, name):\n",
    "\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "\n",
    "    for x in dummies.columns:\n",
    "\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "\n",
    "        df[dummy_name] = dummies[x]\n",
    "\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "encode_text_dummy(multiclass_df,'http.request.method')\n",
    "\n",
    "encode_text_dummy(multiclass_df,'http.referer')\n",
    "\n",
    "encode_text_dummy(multiclass_df,\"http.request.version\")\n",
    "\n",
    "encode_text_dummy(multiclass_df,\"dns.qry.name.len\")\n",
    "\n",
    "encode_text_dummy(multiclass_df,\"mqtt.conack.flags\")\n",
    "\n",
    "encode_text_dummy(multiclass_df,\"mqtt.protoname\")\n",
    "\n",
    "encode_text_dummy(multiclass_df,\"mqtt.topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max index value of binary_df\n",
    "print(\"max index value of binary_df:\", max(multiclass_df.index))\n",
    "print(multiclass_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to drop some unrequired columns from the DF, but we need to keep the original around as we may need to split the data differently for different models based on things like the IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_columns = [\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\",\"arp.dst.proto_ipv4\", \n",
    "drop_columns = [\"frame.time\", \"arp.src.proto_ipv4\",\"arp.dst.proto_ipv4\", \n",
    "\n",
    "         \"http.file_data\",\"http.request.full_uri\",\"icmp.transmit_timestamp\",\n",
    "\n",
    "         \"http.request.uri.query\", \"tcp.options\",\"tcp.payload\",\"tcp.srcport\",\n",
    "\n",
    "         \"tcp.dstport\", \"udp.port\", \"mqtt.msg\"]\n",
    "\n",
    "multiclass_df = multiclass_df.drop(drop_columns, axis=1)\n",
    "\n",
    "multiclass_df = multiclass_df.dropna(axis=0, how='any')\n",
    "\n",
    "multiclass_df = multiclass_df.drop_duplicates(subset=None, keep=\"first\")\n",
    "\n",
    "# We cant shuffle at this point as we need to keep the order so we can split the dataset later based on things like IP address\n",
    "#binary_df_copy = shuffle(binary_df_copy)\n",
    "\n",
    "# Compute the number of missing values (NaN or null) in each column of a pandas DataFrame object named df.\n",
    "multiclass_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max index value of binary_df\n",
    "print(\"max index value of binary_df:\", max(multiclass_df.index))\n",
    "print(multiclass_df.shape)\n",
    "\n",
    "multiclass_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print max index value of binary_df\n",
    "print(\"max index value of binary_df:\", max(multiclass_df.index))\n",
    "print(multiclass_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the Attack Type to be a unique number for the attack type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of Types\n",
    "attacks = {'Normal': 0 ,'Backdoor' :1, 'DDoS_HTTP':2,  'DDoS_ICMP':3, 'DDoS_TCP':4, 'DDoS_UDP':5, \n",
    "           'Fingerprinting':6, 'MITM':7, 'Password':8, 'Port_Scanning':9, 'Ransomware':10, \n",
    "           'SQL_injection':11, 'Uploading':12, 'Vulnerability_scanner':13, 'XSS':14}\n",
    "\n",
    "multiclass_df['Attack_type'] = multiclass_df['Attack_type'].map(attacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = multiclass_df['Attack_type']\n",
    "le = preprocessing.LabelEncoder()\n",
    "label_n = le.fit_transform(label.values)\n",
    "\n",
    "# Stratify based on the attack label to balance the dataset - This is our original copy of the data include IP addresses\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(multiclass_df, label_n, stratify=label_n, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train_df.shape)\n",
    "\n",
    "X_train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#print the max index of X_train_df\n",
    "print(X_train_df.index.max())\n",
    "\n",
    "#  print the max index of binary_df\n",
    "print(binary_df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = binary_df['Attack_label']\n",
    "le = preprocessing.LabelEncoder()\n",
    "label_n = le.fit_transform(label.values)\n",
    "\n",
    "# Stratify based on the attack label to balance the dataset - This is our original copy of the data include IP addresses\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(binary_df, label_n, stratify=label_n, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train_df.shape)\n",
    "\n",
    "#print the max index of X_train_df\n",
    "print(X_train_df.index.max())\n",
    "\n",
    "#  print the max index of binary_df\n",
    "print(binary_df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_df_copy = binary_df.copy()\n",
    "\n",
    "binary_df_copy = binary_df_copy.drop([\"ip.src_host\", \"ip.dst_host\", \"Attack_label\"], axis=1)\n",
    "\n",
    "# This is our copy of the data without IP addresses\n",
    "scaled_features = StandardScaler().fit_transform(binary_df_copy.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, label_n, stratify=label_n, test_size=0.2, random_state=42)\n",
    "\n",
    "print (\"Train:\", X_train.shape, y_train.shape)\n",
    "print (\"Test:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Attack classification\n",
    "N_CLASSES = 2\n",
    "# Number of features = size of binary_df - 1 (Attack_label)\n",
    "N_FEATURES = binary_df.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_X_train = []\n",
    "fl_y_train = []\n",
    "\n",
    "if METHOD == 'STRATIFIED':\n",
    "    # Stratfiy the dataset\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=NUM_OF_STRATIFIED_CLIENTS, shuffle=True, random_state=42)\n",
    "    skf.get_n_splits(X_train, y_train)\n",
    "\n",
    "    for _, train_index in skf.split(X_train, y_train):\n",
    "        print(\"TRAIN:\", train_index)\n",
    "        X_np = X_train[train_index]\n",
    "        y_np = y_train[train_index]\n",
    "\n",
    "        fl_X_train.append(X_np)\n",
    "        fl_y_train.append(y_np)\n",
    "\n",
    "else: # UNUSED\n",
    "    # Individual IP address\n",
    "    for ip in known_sensor_ip_addresses:\n",
    "        new_ip = [ip]\n",
    "        print(\"new_ip:\", new_ip)\n",
    "\n",
    "        X_train_df['ip.src_host']\n",
    "        \n",
    "        print(\"Shape X_Train:\", X_train.shape)\n",
    "        print(\"Shape y_Train:\", y_train.shape)\n",
    "\n",
    "        # Filter dataframe by IP address\n",
    "        new_df_src = X_train_df[ X_train_df['ip.src_host'].isin(new_ip) ]\n",
    "        new_df_dst = X_train_df[ X_train_df['ip.dst_host'].isin(new_ip) ]\n",
    "\n",
    "        print(\"Shape new_df_src:\", new_df_src.shape)\n",
    "        print(\"Shape new_df_dst:\", new_df_dst.shape)\n",
    "\n",
    "        X_np = np.vstack([ X_train[ new_df_src.index, : ], X_train[ new_df_dst.index, :] ])\n",
    "        y_np = np.hstack([ y_train[ new_df_src.index ], y_train[ new_df_dst.index ] ])\n",
    "\n",
    "        print (\"x_np:\", X_np.shape)\n",
    "        print (\"y_np:\", y_np.shape)\n",
    "\n",
    "        fl_X_train.append(X_np)\n",
    "        fl_y_train.append(y_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the size of the fl_X_train\n",
    "print (\"fl_X_train size:\", len(fl_X_train))\n",
    "\n",
    "# Print out the size of each element in the fl_X_train\n",
    "for i in range(len(fl_X_train)):\n",
    "    print (\"fl_X_train[\", i, \"]:\", fl_X_train[i].shape)\n",
    "    print (\"fl_y_train[\", i, \"]:\", fl_y_train[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label.unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_CLIENTS = len(fl_X_train)\n",
    "print(\"NUM_OF_CLIENTS:\", NUM_OF_CLIENTS)    \n",
    "\n",
    "NUM_OF_ROUNDS = 10\n",
    "\n",
    "print(\"Checking data split groups\")\n",
    "for i in range(len(fl_X_train)):\n",
    "    print(i, \":\", \"X Shape\", fl_X_train[i].shape, \"Y Shape\", fl_Y_train[i].shape)\n",
    "\n",
    "print(\"\\nDeploy Simulation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"tf\", tf.__version__)\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "class NumpyFlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, model, train_data, train_labels):\n",
    "        self.model = model\n",
    "        self.cid = cid\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Training...\")\n",
    "        self.model.fit(self.train_data, self.train_labels, epochs=1, batch_size=32)\n",
    "        print (\"Client \", self.cid, \"Training complete...\")\n",
    "        return self.model.get_weights(), len(self.train_data), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Evaluating...\")\n",
    "        loss, accuracy = self.model.evaluate(self.train_data, self.train_labels, batch_size=32)\n",
    "        print (\"Client \", self.cid, \"Evaluating complete...\", accuracy, loss)\n",
    "        return loss, len(self.train_data), {\"accuracy\": accuracy}\n",
    "    \n",
    "    def predict(self, incoming):\n",
    "        prediction = np.argmax( self.model.predict(incoming) ,axis=1)\n",
    "        return prediction\n",
    "\n",
    "def client_fn(cid: str) -> NumpyFlowerClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    #model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n",
    "    #model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print (\"Client ID:\", cid)\n",
    "\n",
    "    model = Sequential([\n",
    "      #Flatten(input_shape=(79,1)),\n",
    "      Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "      Dense(256, activation='sigmoid'),\n",
    "      Dense(128, activation='sigmoid'), \n",
    "      #Dense(18, activation='sigmoid'),  \n",
    "      Dense(len(label.unique()), activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    partition_id = int(cid)\n",
    "    X_train_c = fl_X_train[partition_id]\n",
    "    y_train_c = fl_y_train[partition_id]\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return NumpyFlowerClient(cid, model, X_train_c, y_train_c)\n",
    "\n",
    "\n",
    "print (\"Deploy simulation...\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "eval_count = 0\n",
    "\n",
    "def get_evaluate_fn(server_model):\n",
    "    global eval_count\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "    # The `evaluate` function will be called after every round\n",
    "    \n",
    "    \n",
    "    def evaluate(server_round, parameters, config):\n",
    "        global eval_count\n",
    "        \n",
    "        # Update model with the latest parameters\n",
    "        server_model.set_weights(parameters)\n",
    "        print (\"Server Evaluating...\", eval_count)\n",
    "        loss, accuracy = server_model.evaluate(X_test, y_test)\n",
    "        \n",
    "        y_pred = server_model.predict(X_test)\n",
    "        print (\"Prediction: \", y_pred, y_pred.shape)\n",
    "        #cmatrix = confusion_matrix(y_test, np.rint(y_pred))\n",
    "        #print (\"confusion_matrix:\", cmatrix, cmatrix.shape)\n",
    "                        \n",
    "        print (\"Server Evaluating complete...\", accuracy, loss)\n",
    "        \n",
    "        np.save(\"y_pred-\" + str(eval_count) + \".npy\", y_pred)\n",
    "        #np.save(\"cmatrix-\" + str(eval_count) + \".npy\", cmatrix)\n",
    "        eval_count = eval_count + 1\n",
    "        \n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "    return evaluate\n",
    "\n",
    "\n",
    "\n",
    "server_model = Sequential([\n",
    "      #Flatten(input_shape=(79,1)),\n",
    "      Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "      Dense(256, activation='sigmoid'),\n",
    "      Dense(128, activation='sigmoid'), \n",
    "      #Dense(18, activation='sigmoid'),  \n",
    "      Dense(len(label.unique()), activation='sigmoid')\n",
    "    ])\n",
    "server_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=2, #10,\n",
    "        min_evaluate_clients=2, #5,\n",
    "        min_available_clients=2, #10,\n",
    "        evaluate_fn=get_evaluate_fn(server_model),\n",
    "        #evaluate_metrics_aggregation_fn=weighted_average,\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_OF_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_OF_ROUNDS),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
